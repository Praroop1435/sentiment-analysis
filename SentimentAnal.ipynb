{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_MJK_V8xEJeF",
        "outputId": "6a4b9e70-01d5-4120-962f-49eef1cc314a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (3.5.1)\n",
            "Requirement already satisfied: mlflow in /usr/local/lib/python3.12/dist-packages (3.4.0)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.7)\n",
            "Requirement already satisfied: mlflow-skinny==3.4.0 in /usr/local/lib/python3.12/dist-packages (from mlflow) (3.4.0)\n",
            "Requirement already satisfied: mlflow-tracing==3.4.0 in /usr/local/lib/python3.12/dist-packages (from mlflow) (3.4.0)\n",
            "Requirement already satisfied: Flask<4 in /usr/local/lib/python3.12/dist-packages (from mlflow) (3.1.2)\n",
            "Requirement already satisfied: alembic!=1.10.0,<2 in /usr/local/lib/python3.12/dist-packages (from mlflow) (1.16.5)\n",
            "Requirement already satisfied: cryptography<46,>=43.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow) (43.0.3)\n",
            "Requirement already satisfied: docker<8,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow) (7.1.0)\n",
            "Requirement already satisfied: fastmcp<3,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow) (2.12.3)\n",
            "Requirement already satisfied: graphene<4 in /usr/local/lib/python3.12/dist-packages (from mlflow) (3.4.3)\n",
            "Requirement already satisfied: gunicorn<24 in /usr/local/lib/python3.12/dist-packages (from mlflow) (23.0.0)\n",
            "Requirement already satisfied: matplotlib<4 in /usr/local/lib/python3.12/dist-packages (from mlflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<3 in /usr/local/lib/python3.12/dist-packages (from mlflow) (2.0.2)\n",
            "Requirement already satisfied: pandas<3 in /usr/local/lib/python3.12/dist-packages (from mlflow) (2.2.2)\n",
            "Requirement already satisfied: pyarrow<22,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow) (18.1.0)\n",
            "Requirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.12/dist-packages (from mlflow) (1.6.1)\n",
            "Requirement already satisfied: scipy<2 in /usr/local/lib/python3.12/dist-packages (from mlflow) (1.16.2)\n",
            "Requirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from mlflow) (2.0.43)\n",
            "Requirement already satisfied: cachetools<7,>=5.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.4.0->mlflow) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.4.0->mlflow) (8.2.1)\n",
            "Requirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.4.0->mlflow) (3.1.1)\n",
            "Requirement already satisfied: databricks-sdk<1,>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.4.0->mlflow) (0.66.0)\n",
            "Requirement already satisfied: fastapi<1 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.4.0->mlflow) (0.116.2)\n",
            "Requirement already satisfied: gitpython<4,>=3.1.9 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.4.0->mlflow) (3.1.45)\n",
            "Requirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.4.0->mlflow) (8.7.0)\n",
            "Requirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.4.0->mlflow) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-proto<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.4.0->mlflow) (1.37.0)\n",
            "Requirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.4.0->mlflow) (1.37.0)\n",
            "Requirement already satisfied: packaging<26 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.4.0->mlflow) (25.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.12.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.4.0->mlflow) (5.29.5)\n",
            "Requirement already satisfied: pydantic<3,>=1.10.8 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.4.0->mlflow) (2.11.9)\n",
            "Requirement already satisfied: python-dotenv<2,>=0.19.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.4.0->mlflow) (1.1.1)\n",
            "Requirement already satisfied: pyyaml<7,>=5.1 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.4.0->mlflow) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.4.0->mlflow) (2.32.4)\n",
            "Requirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.4.0->mlflow) (0.5.3)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.4.0->mlflow) (4.15.0)\n",
            "Requirement already satisfied: uvicorn<1 in /usr/local/lib/python3.12/dist-packages (from mlflow-skinny==3.4.0->mlflow) (0.35.0)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.12/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.3.10)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography<46,>=43.0.0->mlflow) (2.0.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from docker<8,>=4.0.0->mlflow) (2.5.0)\n",
            "Requirement already satisfied: authlib>=1.5.2 in /usr/local/lib/python3.12/dist-packages (from fastmcp<3,>=2.0.0->mlflow) (1.6.4)\n",
            "Requirement already satisfied: cyclopts>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from fastmcp<3,>=2.0.0->mlflow) (3.24.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.2.2 in /usr/local/lib/python3.12/dist-packages (from fastmcp<3,>=2.0.0->mlflow) (1.3.0)\n",
            "Requirement already satisfied: httpx>=0.28.1 in /usr/local/lib/python3.12/dist-packages (from fastmcp<3,>=2.0.0->mlflow) (0.28.1)\n",
            "Requirement already satisfied: mcp<2.0.0,>=1.12.4 in /usr/local/lib/python3.12/dist-packages (from fastmcp<3,>=2.0.0->mlflow) (1.14.1)\n",
            "Requirement already satisfied: openapi-core>=0.19.5 in /usr/local/lib/python3.12/dist-packages (from fastmcp<3,>=2.0.0->mlflow) (0.19.5)\n",
            "Requirement already satisfied: openapi-pydantic>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from fastmcp<3,>=2.0.0->mlflow) (0.5.1)\n",
            "Requirement already satisfied: pyperclip>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from fastmcp<3,>=2.0.0->mlflow) (1.10.0)\n",
            "Requirement already satisfied: rich>=13.9.4 in /usr/local/lib/python3.12/dist-packages (from fastmcp<3,>=2.0.0->mlflow) (13.9.4)\n",
            "Requirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (1.9.0)\n",
            "Requirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (2.2.0)\n",
            "Requirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (3.1.6)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (3.0.2)\n",
            "Requirement already satisfied: werkzeug>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from Flask<4->mlflow) (3.1.1)\n",
            "Requirement already satisfied: graphql-core<3.3,>=3.1 in /usr/local/lib/python3.12/dist-packages (from graphene<4->mlflow) (3.2.6)\n",
            "Requirement already satisfied: graphql-relay<3.3,>=3.1 in /usr/local/lib/python3.12/dist-packages (from graphene<4->mlflow) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil<3,>=2.7.0 in /usr/local/lib/python3.12/dist-packages (from graphene<4->mlflow) (2.9.0.post0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (4.60.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib<4->mlflow) (3.2.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3->mlflow) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3->mlflow) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2->mlflow) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2->mlflow) (3.6.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.2.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography<46,>=43.0.0->mlflow) (2.23)\n",
            "Requirement already satisfied: attrs>=23.1.0 in /usr/local/lib/python3.12/dist-packages (from cyclopts>=3.0.0->fastmcp<3,>=2.0.0->mlflow) (25.3.0)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.12/dist-packages (from cyclopts>=3.0.0->fastmcp<3,>=2.0.0->mlflow) (0.17.0)\n",
            "Requirement already satisfied: rich-rst<2.0.0,>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from cyclopts>=3.0.0->fastmcp<3,>=2.0.0->mlflow) (1.3.1)\n",
            "Requirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.12/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==3.4.0->mlflow) (2.38.0)\n",
            "Requirement already satisfied: starlette<0.49.0,>=0.40.0 in /usr/local/lib/python3.12/dist-packages (from fastapi<1->mlflow-skinny==3.4.0->mlflow) (0.48.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython<4,>=3.1.9->mlflow-skinny==3.4.0->mlflow) (4.0.12)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->fastmcp<3,>=2.0.0->mlflow) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->fastmcp<3,>=2.0.0->mlflow) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->fastmcp<3,>=2.0.0->mlflow) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.28.1->fastmcp<3,>=2.0.0->mlflow) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.28.1->fastmcp<3,>=2.0.0->mlflow) (0.16.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==3.4.0->mlflow) (3.23.0)\n",
            "Requirement already satisfied: httpx-sse>=0.4 in /usr/local/lib/python3.12/dist-packages (from mcp<2.0.0,>=1.12.4->fastmcp<3,>=2.0.0->mlflow) (0.4.1)\n",
            "Requirement already satisfied: jsonschema>=4.20.0 in /usr/local/lib/python3.12/dist-packages (from mcp<2.0.0,>=1.12.4->fastmcp<3,>=2.0.0->mlflow) (4.25.1)\n",
            "Requirement already satisfied: pydantic-settings>=2.5.2 in /usr/local/lib/python3.12/dist-packages (from mcp<2.0.0,>=1.12.4->fastmcp<3,>=2.0.0->mlflow) (2.10.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.12/dist-packages (from mcp<2.0.0,>=1.12.4->fastmcp<3,>=2.0.0->mlflow) (0.0.20)\n",
            "Requirement already satisfied: sse-starlette>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from mcp<2.0.0,>=1.12.4->fastmcp<3,>=2.0.0->mlflow) (3.0.2)\n",
            "Requirement already satisfied: isodate in /usr/local/lib/python3.12/dist-packages (from openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow) (0.7.2)\n",
            "Requirement already satisfied: jsonschema-path<0.4.0,>=0.3.1 in /usr/local/lib/python3.12/dist-packages (from openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow) (0.3.4)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow) (10.8.0)\n",
            "Requirement already satisfied: openapi-schema-validator<0.7.0,>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow) (0.6.3)\n",
            "Requirement already satisfied: openapi-spec-validator<0.8.0,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow) (0.7.2)\n",
            "Requirement already satisfied: parse in /usr/local/lib/python3.12/dist-packages (from openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow) (1.20.2)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.58b0 in /usr/local/lib/python3.12/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==3.4.0->mlflow) (0.58b0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.4.0->mlflow) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.4.0->mlflow) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==3.4.0->mlflow) (0.4.1)\n",
            "Requirement already satisfied: email-validator>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pydantic[email]>=2.11.7->fastmcp<3,>=2.0.0->mlflow) (2.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil<3,>=2.7.0->graphene<4->mlflow) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==3.4.0->mlflow) (3.4.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.9.4->fastmcp<3,>=2.0.0->mlflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.9.4->fastmcp<3,>=2.0.0->mlflow) (2.19.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.28.1->fastmcp<3,>=2.0.0->mlflow) (1.3.1)\n",
            "Requirement already satisfied: dnspython>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from email-validator>=2.0.0->pydantic[email]>=2.11.7->fastmcp<3,>=2.0.0->mlflow) (2.8.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython<4,>=3.1.9->mlflow-skinny==3.4.0->mlflow) (5.0.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.4.0->mlflow) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.4.0->mlflow) (4.9.1)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.12.4->fastmcp<3,>=2.0.0->mlflow) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.12.4->fastmcp<3,>=2.0.0->mlflow) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.12.4->fastmcp<3,>=2.0.0->mlflow) (0.27.1)\n",
            "Requirement already satisfied: pathable<0.5.0,>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema-path<0.4.0,>=0.3.1->openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow) (0.4.4)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=13.9.4->fastmcp<3,>=2.0.0->mlflow) (0.1.2)\n",
            "Requirement already satisfied: rfc3339-validator in /usr/local/lib/python3.12/dist-packages (from openapi-schema-validator<0.7.0,>=0.6.0->openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow) (0.1.4)\n",
            "Requirement already satisfied: lazy-object-proxy<2.0.0,>=1.7.1 in /usr/local/lib/python3.12/dist-packages (from openapi-spec-validator<0.8.0,>=0.7.1->openapi-core>=0.19.5->fastmcp<3,>=2.0.0->mlflow) (1.12.0)\n",
            "Requirement already satisfied: docutils in /usr/local/lib/python3.12/dist-packages (from rich-rst<2.0.0,>=1.3.1->cyclopts>=3.0.0->fastmcp<3,>=2.0.0->mlflow) (0.21.2)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==3.4.0->mlflow) (0.6.1)\n",
            "Collecting dagshub\n",
            "  Downloading dagshub-0.6.3-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: PyYAML>=5 in /usr/local/lib/python3.12/dist-packages (from dagshub) (6.0.2)\n",
            "Collecting appdirs>=1.4.4 (from dagshub)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: click>=8.0.4 in /usr/local/lib/python3.12/dist-packages (from dagshub) (8.2.1)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from dagshub) (0.28.1)\n",
            "Requirement already satisfied: GitPython>=3.1.29 in /usr/local/lib/python3.12/dist-packages (from dagshub) (3.1.45)\n",
            "Requirement already satisfied: rich>=13.1.0 in /usr/local/lib/python3.12/dist-packages (from dagshub) (13.9.4)\n",
            "Collecting dacite~=1.6.0 (from dagshub)\n",
            "  Downloading dacite-1.6.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: tenacity>=8.2.2 in /usr/local/lib/python3.12/dist-packages (from dagshub) (8.5.0)\n",
            "Collecting gql[requests] (from dagshub)\n",
            "  Downloading gql-4.0.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting dataclasses-json (from dagshub)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from dagshub) (2.2.2)\n",
            "Collecting treelib>=1.6.4 (from dagshub)\n",
            "  Downloading treelib-1.8.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting pathvalidate>=3.0.0 (from dagshub)\n",
            "  Downloading pathvalidate-3.3.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (from dagshub) (2.9.0.post0)\n",
            "Collecting boto3 (from dagshub)\n",
            "  Downloading boto3-1.40.38-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting semver (from dagshub)\n",
            "  Downloading semver-3.0.4-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting dagshub-annotation-converter>=0.1.12 (from dagshub)\n",
            "  Downloading dagshub_annotation_converter-0.1.13-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from dagshub-annotation-converter>=0.1.12->dagshub) (5.4.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from dagshub-annotation-converter>=0.1.12->dagshub) (11.3.0)\n",
            "Requirement already satisfied: pydantic>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from dagshub-annotation-converter>=0.1.12->dagshub) (2.11.9)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from dagshub-annotation-converter>=0.1.12->dagshub) (4.15.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from GitPython>=3.1.29->dagshub) (4.0.12)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.23.0->dagshub) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.23.0->dagshub) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.23.0->dagshub) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.23.0->dagshub) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.23.0->dagshub) (0.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.1.0->dagshub) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.1.0->dagshub) (2.19.2)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from treelib>=1.6.4->dagshub) (1.17.0)\n",
            "Collecting botocore<1.41.0,>=1.40.38 (from boto3->dagshub)\n",
            "  Downloading botocore-1.40.38-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->dagshub)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.15.0,>=0.14.0 (from boto3->dagshub)\n",
            "  Downloading s3transfer-0.14.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json->dagshub)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json->dagshub)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: graphql-core<3.3,>=3.2 in /usr/local/lib/python3.12/dist-packages (from gql[requests]->dagshub) (3.2.6)\n",
            "Requirement already satisfied: yarl<2.0,>=1.6 in /usr/local/lib/python3.12/dist-packages (from gql[requests]->dagshub) (1.20.1)\n",
            "Collecting backoff<3.0,>=1.11.1 (from gql[requests]->dagshub)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: requests<3,>=2.26 in /usr/local/lib/python3.12/dist-packages (from gql[requests]->dagshub) (2.32.4)\n",
            "Requirement already satisfied: requests_toolbelt<2,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from gql[requests]->dagshub) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas->dagshub) (2.0.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->dagshub) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->dagshub) (2025.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.23.0->dagshub) (1.3.1)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.12/dist-packages (from botocore<1.41.0,>=1.40.38->boto3->dagshub) (2.5.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->GitPython>=3.1.29->dagshub) (5.0.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=13.1.0->dagshub) (0.1.2)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.12/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->dagshub) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0.0->dagshub-annotation-converter>=0.1.12->dagshub) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0.0->dagshub-annotation-converter>=0.1.12->dagshub) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.0.0->dagshub-annotation-converter>=0.1.12->dagshub) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.26->gql[requests]->dagshub) (3.4.3)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json->dagshub)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: multidict>=4.0 in /usr/local/lib/python3.12/dist-packages (from yarl<2.0,>=1.6->gql[requests]->dagshub) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from yarl<2.0,>=1.6->gql[requests]->dagshub) (0.3.2)\n",
            "Downloading dagshub-0.6.3-py3-none-any.whl (261 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.3/261.3 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading dacite-1.6.0-py3-none-any.whl (12 kB)\n",
            "Downloading dagshub_annotation_converter-0.1.13-py3-none-any.whl (35 kB)\n",
            "Downloading pathvalidate-3.3.1-py3-none-any.whl (24 kB)\n",
            "Downloading treelib-1.8.0-py3-none-any.whl (30 kB)\n",
            "Downloading boto3-1.40.38-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading semver-3.0.4-py3-none-any.whl (17 kB)\n",
            "Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading botocore-1.40.38-py3-none-any.whl (14.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m92.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading s3transfer-0.14.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.7/85.7 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading gql-4.0.0-py3-none-any.whl (89 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.9/89.9 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: appdirs, treelib, semver, pathvalidate, mypy-extensions, marshmallow, jmespath, dacite, backoff, typing-inspect, gql, botocore, s3transfer, dataclasses-json, dagshub-annotation-converter, boto3, dagshub\n",
            "Successfully installed appdirs-1.4.4 backoff-2.2.1 boto3-1.40.38 botocore-1.40.38 dacite-1.6.0 dagshub-0.6.3 dagshub-annotation-converter-0.1.13 dataclasses-json-0.6.7 gql-4.0.0 jmespath-1.0.1 marshmallow-3.26.1 mypy-extensions-1.1.0 pathvalidate-3.3.1 s3transfer-0.14.0 semver-3.0.4 treelib-1.8.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "!pip install pyspark mlflow\n",
        "import kagglehub\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import Tokenizer, StopWordsRemover, HashingTF, IDF\n",
        "import pyspark.sql.functions as F\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiY4kr5qEYZx",
        "outputId": "c127f955-f403-4a5b-b33d-34af6c0067cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'amazonreviews' dataset.\n",
            "Path to dataset files: /kaggle/input/amazonreviews\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"bittlingmayer/amazonreviews\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnjlFKzhEaiX",
        "outputId": "a89857a8-fa40-4aed-f40c-f235ac07664a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---------+\n",
            "|label|     text|\n",
            "+-----+---------+\n",
            "|    1|  Stuning|\n",
            "|    1|      The|\n",
            "|    1|Amazing!:|\n",
            "|    1|Excellent|\n",
            "|    1|Remember,|\n",
            "+-----+---------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import pyspark.sql.functions as F\n",
        "import mlflow\n",
        "\n",
        "# Initialize Spark\n",
        "spark = SparkSession.builder.appName(\"AmazonSentiment\").getOrCreate()\n",
        "\n",
        "# Load and parse the data (assuming file is at /content/train.ft.txt.bz2)\n",
        "df = spark.read.option(\"delimiter\", \" \").csv(\"/kaggle/input/amazonreviews/train.ft.txt.bz2\")\n",
        "df = df.withColumn(\"label\", F.when(F.col(\"_c0\") == \"__label__2\", 1).otherwise(0)) \\\n",
        "       .withColumn(\"text\", F.col(\"_c1\")) \\\n",
        "       .select(\"label\", \"text\")\n",
        "\n",
        "df.show(5, truncate=50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8n02R1ZNxZ4"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Using the full dataset\n",
        "(train_data, test_data) = df.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# --- SOLUTION 1: CACHE THE DATA ---\n",
        "train_data.cache()\n",
        "test_data.cache()\n",
        "\n",
        "# Define pipeline stages\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
        "hashingTF = HashingTF(inputCol=\"filtered_words\", outputCol=\"rawFeatures\", numFeatures=20000)\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
        "\n",
        "# Create the full pipeline\n",
        "full_pipeline = Pipeline(stages=[tokenizer, remover, hashingTF, idf, lr])\n",
        "\n",
        "with mlflow.start_run():\n",
        "    mlflow.log_param(\"regParam\", lr.getRegParam())\n",
        "    mlflow.log_param(\"maxIter\", lr.getMaxIter())\n",
        "\n",
        "    # Train the model\n",
        "    print(\"Fitting the model...\")\n",
        "    model = full_pipeline.fit(train_data)\n",
        "\n",
        "    # Make predictions\n",
        "    print(\"Transforming test data...\")\n",
        "    predictions = model.transform(test_data)\n",
        "\n",
        "    # --- SOLUTION 1: CACHE PREDICTIONS ---\n",
        "    predictions.cache()\n",
        "    print(\"Predictions cached.\")\n",
        "\n",
        "    # --- LOG METRICS ---\n",
        "    print(\"Evaluating metrics...\")\n",
        "    evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\")\n",
        "    auc = evaluator.evaluate(predictions, {evaluator.metricName: \"areaUnderROC\"})\n",
        "\n",
        "    # --- SOLUTION 2: EFFICIENT ACCURACY CALCULATION ---\n",
        "    # Cast boolean to integer (true=1, false=0) and take the average\n",
        "    accuracy_df = predictions.withColumn('correct', F.when(F.col('label') == F.col('prediction'), 1).otherwise(0))\n",
        "    accuracy = accuracy_df.select(F.avg('correct')).first()[0]\n",
        "\n",
        "    mlflow.log_metric(\"AUC\", auc)\n",
        "    mlflow.log_metric(\"Accuracy\", accuracy)\n",
        "\n",
        "    # Unpersist the cached dataframes to free up memory\n",
        "    train_data.unpersist()\n",
        "    test_data.unpersist()\n",
        "    predictions.unpersist()\n",
        "\n",
        "    # --- LOG MODEL ---\n",
        "    print(\"Logging model...\")\n",
        "    mlflow.spark.log_model(model, \"spark-lr-model\")\n",
        "\n",
        "    print(f\"Model logged! AUC: {auc:.4f}, Accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHaLMrFga3Ce",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import mlflow\n",
        "from pyspark.ml import Pipeline\n",
        "from pyspark.ml.feature import (\n",
        "    Tokenizer, StopWordsRemover, HashingTF, IDF, NGram, VectorAssembler\n",
        ")\n",
        "# Make sure to import the model you are actually using\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "# --- Use a small data sample for testing ---\n",
        "# Ensure df and spark are defined from your earlier cells\n",
        "df_sampled = df.sample(fraction=0.01, seed=42)\n",
        "(train_data, test_data) = df_sampled.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "# --- 1. Define Feature Engineering Stages ---\n",
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
        "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
        "ngram = NGram(n=2, inputCol=\"filtered_words\", outputCol=\"ngrams\")\n",
        "hashingTF_words = HashingTF(inputCol=\"filtered_words\", outputCol=\"hashed_words\", numFeatures=5000)\n",
        "hashingTF_ngrams = HashingTF(inputCol=\"ngrams\", outputCol=\"hashed_ngrams\", numFeatures=5000)\n",
        "assembler = VectorAssembler(inputCols=[\"hashed_words\", \"hashed_ngrams\"], outputCol=\"rawFeatures\")\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n",
        "\n",
        "# --- 2. Define the Model (use the fast one for testing) ---\n",
        "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
        "\n",
        "# --- 3. Assemble the Full Pipeline (define it only ONCE) ---\n",
        "pipeline = Pipeline(stages=[\n",
        "    tokenizer, remover, ngram,\n",
        "    hashingTF_words, hashingTF_ngrams,\n",
        "    assembler, idf,\n",
        "    lr  # <-- Use the faster model in the pipeline\n",
        "])\n",
        "\n",
        "# --- 4. Set up Hyperparameter Tuning ---\n",
        "paramGrid = (ParamGridBuilder()\n",
        "             .addGrid(lr.regParam, [0.1, 0.01])  # Add a second option to see tuning work\n",
        "             .addGrid(lr.maxIter, [10])\n",
        "             .build())\n",
        "\n",
        "evaluator = BinaryClassificationEvaluator(rawPredictionCol=\"rawPrediction\", labelCol=\"label\", metricName=\"areaUnderROC\")\n",
        "\n",
        "crossval = CrossValidator(estimator=pipeline,\n",
        "                          estimatorParamMaps=paramGrid,\n",
        "                          evaluator=evaluator,\n",
        "                          numFolds=2)\n",
        "\n",
        "# --- 5. Run Training and Log with MLflow ---\n",
        "with mlflow.start_run():\n",
        "    print(\"Starting cross-validation...\")\n",
        "    cvModel = crossval.fit(train_data)\n",
        "    print(\"Cross-validation complete.\")\n",
        "\n",
        "    best_model = cvModel.bestModel\n",
        "    predictions = best_model.transform(test_data)\n",
        "\n",
        "    auc = evaluator.evaluate(predictions)\n",
        "    accuracy = predictions.filter(predictions.label == predictions.prediction).count() / float(predictions.count())\n",
        "\n",
        "    print(f\"Best Model Found! AUC: {auc:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    # --- FIX: Log the CORRECT parameters for the LogisticRegression model ---\n",
        "    best_lr_model = best_model.stages[-1]\n",
        "    mlflow.log_param(\"best_regParam\", best_lr_model.getRegParam())\n",
        "    mlflow.log_param(\"best_maxIter\", best_lr_model.getMaxIter())\n",
        "\n",
        "    mlflow.log_metric(\"AUC\", auc)\n",
        "    mlflow.log_metric(\"Accuracy\", accuracy)\n",
        "\n",
        "    input_example = test_data.limit(5).toPandas()\n",
        "    mlflow.spark.log_model(\n",
        "        best_model,\n",
        "        \"spark-lr-cv-model\",\n",
        "        input_example=input_example\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3BidkfjiaY1v"
      },
      "outputs": [],
      "source": [
        "# # Install the pyngrok library\n",
        "# !pip install pyngrok\n",
        "\n",
        "# # Kill any existing mlflow and ngrok processes to start fresh\n",
        "# !killall mlflow\n",
        "# !killall ngrok\n",
        "\n",
        "# from pyngrok import ngrok\n",
        "# import os\n",
        "\n",
        "# # Terminate open tunnels if any exist\n",
        "# ngrok.kill()\n",
        "\n",
        "# # Set your ngrok authtoken (optional but recommended, get one from ngrok.com)\n",
        "# ngrok.set_auth_token(\"338tbSnswQ3CUcZHfDrk8s0z4ra_7me8sxm24gMs6nV73Txon\")\n",
        "\n",
        "# # Set the MLFLOW_TRACKING_URI so MLflow knows where to store experiments\n",
        "# # This will create an mlruns directory in your Colab environment\n",
        "# os.environ[\"MLFLOW_TRACKING_URI\"] = \"mlruns\"\n",
        "\n",
        "# # Run MLflow UI in the background\n",
        "# get_ipython().system_raw(\"mlflow ui --port 5000 &\")\n",
        "\n",
        "# # Create a public URL to the local port 5000\n",
        "# public_url = ngrok.connect(5000)\n",
        "# print(\"✅ MLflow UI is running. Access it at:\")\n",
        "# print(public_url)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import mlflow\n",
        "# import tensorflow as tf\n",
        "# from tensorflow.keras.models import Sequential\n",
        "# from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, TextVectorization, Dropout\n",
        "# from tensorflow.keras.callbacks import EarlyStopping\n",
        "# import numpy as np\n",
        "# import re\n",
        "# import string\n",
        "# # --- NLTK Imports for Lemmatization ---\n",
        "# import nltk\n",
        "# from nltk.stem import WordNetLemmatizer\n",
        "# from nltk.tokenize import word_tokenize\n",
        "\n",
        "# # --- Download NLTK data (only needs to be done once) ---\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('wordnet')\n",
        "\n",
        "# # --- Connect to DAGsHub (ensure this is configured) ---\n",
        "# import dagshub\n",
        "# dagshub.init(repo_owner='Praproop14_35', repo_name='sentiment-analysis', mlflow=True) # I corrected your username based on previous screenshots\n",
        "# mlflow.set_experiment(\"Optimized-BiLSTM-with-Lemma\")\n",
        "# mlflow.tensorflow.autolog()\n",
        "\n",
        "# # --- 1. Upgraded Text Cleaning with Lemmatization ---\n",
        "# lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# def clean_text_and_lemmatize(text):\n",
        "#     text = text.lower()\n",
        "#     text = re.sub(r'<.*?>', '', text)\n",
        "#     text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)\n",
        "#     text = re.sub(r'\\d+', '', text)\n",
        "#     text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "#     # --- NEW: Tokenize and Lemmatize ---\n",
        "#     tokens = word_tokenize(text)\n",
        "#     lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "#     return \" \".join(lemmatized_tokens)\n",
        "\n",
        "# # Prepare data and apply the new cleaning function\n",
        "# df_sampled = df.sample(fraction=0.02, seed=42)\n",
        "# train_pdf = df_sampled.toPandas()\n",
        "# train_pdf['text'] = train_pdf['text'].apply(clean_text_and_lemmatize)\n",
        "\n",
        "# X_train = train_pdf['text'].to_numpy()\n",
        "# y_train = train_pdf['label'].to_numpy()\n",
        "\n",
        "# # --- 2. Build the Keras Model (architecture is the same) ---\n",
        "# max_features = 10000\n",
        "# sequence_length = 250\n",
        "# vectorize_layer = TextVectorization(\n",
        "#     max_tokens=max_features,\n",
        "#     output_mode='int',\n",
        "#     output_sequence_length=sequence_length)\n",
        "# vectorize_layer.adapt(X_train)\n",
        "\n",
        "# model = Sequential([\n",
        "#     vectorize_layer,\n",
        "#     Embedding(max_features + 1, 256),\n",
        "#     Bidirectional(LSTM(128, return_sequences=True)),\n",
        "#     Dropout(0.3),\n",
        "#     Bidirectional(LSTM(64)),\n",
        "#     Dropout(0.3),\n",
        "#     Dense(64, activation='relu'),\n",
        "#     Dense(1, activation='sigmoid')\n",
        "# ])\n",
        "\n",
        "# model.compile(optimizer='adam',\n",
        "#               loss='binary_crossentropy',\n",
        "#               metrics=['accuracy'])\n",
        "\n",
        "# # --- 3. Refine the Training Process ---\n",
        "# early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# with mlflow.start_run():\n",
        "#     print(\"Starting optimized BiLSTM model training with lemmatization...\")\n",
        "\n",
        "#     model.fit(X_train, y_train,\n",
        "#               epochs=15,\n",
        "#               validation_split=0.2,\n",
        "#               batch_size=32,\n",
        "#               callbacks=[early_stopping])\n",
        "\n",
        "#     print(\"\\nTraining complete.\")"
      ],
      "metadata": {
        "id": "k_L9K60VT4qF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import os\n",
        "import mlflow\n",
        "import mlflow\n",
        "\n",
        "# Set the tracking URI to Databricks\n",
        "mlflow.set_tracking_uri(\"databricks\")\n",
        "\n",
        "# Set the experiment by its path\n",
        "mlflow.set_experiment(\"/Users/anandpraroop@gmail.com/BiLSTM-Colab-Experiment\")\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "id": "W5Iru-5h5q2K",
        "outputId": "d1172097-1697-4b02-d82d-eec7f76ea6a9"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MlflowException",
          "evalue": "Reading Databricks credential configuration failed with MLflow tracking URI 'databricks'. Please ensure that the 'databricks-sdk' PyPI library is installed, the tracking URI is set correctly, and Databricks authentication is properly configured. The tracking URI can be either 'databricks' (using profile name specified by 'DATABRICKS_CONFIG_PROFILE' environment variable or using 'DEFAULT' authentication profile if 'DATABRICKS_CONFIG_PROFILE' environment variable does not exist) or 'databricks://{profile}'. You can configure Databricks authentication in several ways, for example by specifying environment variables (e.g. DATABRICKS_HOST + DATABRICKS_TOKEN) or logging in using 'databricks auth login'. \nFor details on configuring Databricks authentication, please refer to 'https://docs.databricks.com/en/dev-tools/auth/index.html#unified-auth'.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMlflowException\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3930338611.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Set the experiment by its path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/Users/anandpraroop@gmail.com/BiLSTM-Colab-Experiment\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/mlflow/tracking/fluent.py\u001b[0m in \u001b[0;36mset_experiment\u001b[0;34m(experiment_name, experiment_id)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0m_experiment_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexperiment_id\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m             \u001b[0mexperiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_experiment_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 _logger.info(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/mlflow/tracking/_tracking_service/client.py\u001b[0m in \u001b[0;36mget_experiment_by_name\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    277\u001b[0m             \u001b[0;34m:\u001b[0m\u001b[0mpy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mmlflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExperiment\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \"\"\"\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_experiment_by_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mrecord_usage_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCreateExperimentEvent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/mlflow/store/tracking/rest_store.py\u001b[0m in \u001b[0;36mget_experiment_by_name\u001b[0;34m(self, experiment_name)\u001b[0m\n\u001b[1;32m    853\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m             \u001b[0mreq_body\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessage_to_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGetExperimentByName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiment_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexperiment_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m             \u001b[0mresponse_proto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_endpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGetExperimentByName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq_body\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mExperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_proto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_proto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mMlflowException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/mlflow/store/tracking/rest_store.py\u001b[0m in \u001b[0;36m_call_endpoint\u001b[0;34m(self, api, json_body, endpoint, retry_timeout_seconds)\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0mresponse_proto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         return call_endpoint(\n\u001b[0;32m--> 201\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_host_creds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m             \u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/mlflow/utils/databricks_utils.py\u001b[0m in \u001b[0;36mget_databricks_host_creds\u001b[0;34m(server_uri)\u001b[0m\n\u001b[1;32m    824\u001b[0m         \u001b[0mdatabricks_auth_profile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 826\u001b[0;31m     \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_databricks_creds_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserver_uri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/mlflow/utils/databricks_utils.py\u001b[0m in \u001b[0;36m_get_databricks_creds_config\u001b[0;34m(tracking_uri)\u001b[0m\n\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m         \u001b[0m_fail_malformed_databricks_auth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtracking_uri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/mlflow/utils/databricks_utils.py\u001b[0m in \u001b[0;36m_fail_malformed_databricks_auth\u001b[0;34m(uri)\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0;34mf\"Additional debug info: the MLflow {uri_name} was set to '{uri}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m         )\n\u001b[0;32m--> 686\u001b[0;31m     raise MlflowException(\n\u001b[0m\u001b[1;32m    687\u001b[0m         \u001b[0;34mf\"Reading Databricks credential configuration failed with MLflow {uri_name} '{uri}'. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;34m\"Please ensure that the 'databricks-sdk' PyPI library is installed, the tracking \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMlflowException\u001b[0m: Reading Databricks credential configuration failed with MLflow tracking URI 'databricks'. Please ensure that the 'databricks-sdk' PyPI library is installed, the tracking URI is set correctly, and Databricks authentication is properly configured. The tracking URI can be either 'databricks' (using profile name specified by 'DATABRICKS_CONFIG_PROFILE' environment variable or using 'DEFAULT' authentication profile if 'DATABRICKS_CONFIG_PROFILE' environment variable does not exist) or 'databricks://{profile}'. You can configure Databricks authentication in several ways, for example by specifying environment variables (e.g. DATABRICKS_HOST + DATABRICKS_TOKEN) or logging in using 'databricks auth login'. \nFor details on configuring Databricks authentication, please refer to 'https://docs.databricks.com/en/dev-tools/auth/index.html#unified-auth'."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 634
        },
        "id": "EsVZeMlO7R88",
        "collapsed": true,
        "outputId": "0bb1067d-1738-4426-f074-06992ffd9050"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Libraries installed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025/09/25 11:12:10 INFO mlflow.tracking.fluent: Experiment with name 'BiLSTM-Local-Training' does not exist. Creating a new experiment.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully connected to MLflow tracking URI: file:///mlruns\n",
            "MLflow experiment set to: BiLSTM-Local-Training\n",
            "Downloading data from Kaggle...\n",
            "Using Colab cache for faster access to the 'amazonreviews' dataset.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ParserError",
          "evalue": "Error tokenizing data. C error: Expected 81 fields in line 2, saw 98\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2239885659.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# Read the bzip2 compressed file into a pandas DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mbz2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbz2_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mdf_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStringIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbz2_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m# Pre-process the raw data and clean column names\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 626\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1921\u001b[0m                     \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1922\u001b[0m                     \u001b[0mcol_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m                 \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1924\u001b[0m                     \u001b[0mnrows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1925\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    233\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mparsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 81 fields in line 2, saw 98\n"
          ]
        }
      ],
      "source": [
        "# Google Colab Notebook\n",
        "# This notebook demonstrates how to train a Bidirectional LSTM model and track\n",
        "# the results in a Databricks MLflow server from a Google Colab environment.\n",
        "\n",
        "# COMMAND ----------\n",
        "# Cell 1: Install Libraries and Setup\n",
        "# Install necessary libraries for Kaggle data access, MLflow, and Databricks integration.\n",
        "\n",
        "!pip install -q kagglehub databricks-cli mlflow tensorflow\n",
        "print(\"Libraries installed.\")\n",
        "\n",
        "# COMMAND ----------\n",
        "# Cell 2: Setup Local MLflow Tracking\n",
        "#\n",
        "# This cell configures MLflow to log runs to the local file system.\n",
        "# No Databricks host or token is needed.\n",
        "\n",
        "import mlflow\n",
        "import os\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense, TextVectorization\n",
        "import numpy as np\n",
        "from google.colab import userdata\n",
        "\n",
        "# MLflow will now log all runs and artifacts to a local directory called 'mlruns'.\n",
        "# The tracking URI is explicitly set to the local file system to make this clear.\n",
        "mlflow.set_tracking_uri(\"file:///mlruns\")\n",
        "\n",
        "# Set a unique experiment name for your local runs.\n",
        "mlflow.set_experiment(\"BiLSTM-Local-Training\")\n",
        "print(f\"Successfully connected to MLflow tracking URI: {mlflow.get_tracking_uri()}\")\n",
        "print(f\"MLflow experiment set to: BiLSTM-Local-Training\")\n",
        "\n",
        "# COMMAND ----------\n",
        "# Cell 3: Data Loading from Kaggle\n",
        "#\n",
        "# This cell uses kagglehub to directly download the data.\n",
        "\n",
        "import kagglehub\n",
        "import pandas as pd\n",
        "import bz2\n",
        "import io\n",
        "\n",
        "print(\"Downloading data from Kaggle...\")\n",
        "\n",
        "# Download the dataset\n",
        "path = kagglehub.dataset_download(\"bittlingmayer/amazonreviews\")\n",
        "file_path = os.path.join(path, \"train.ft.txt.bz2\")\n",
        "\n",
        "# Read the bzip2 compressed file into a pandas DataFrame\n",
        "with bz2.open(file_path, \"rt\", encoding=\"utf-8\") as bz2_file:\n",
        "    df_raw = pd.read_csv(io.StringIO(bz2_file.read()), sep=\" \", header=None)\n",
        "\n",
        "# Pre-process the raw data and clean column names\n",
        "df_raw.columns = ['label', 'text']\n",
        "df_raw['label'] = df_raw['label'].str.replace('__label__', '').astype(int) - 1\n",
        "df_raw['text'] = df_raw['text'].fillna('')\n",
        "df_raw = df_raw[['label', 'text']]\n",
        "\n",
        "print(\"Data loaded successfully.\")\n",
        "print(\"Sample of data:\")\n",
        "print(df_raw.head())\n",
        "\n",
        "# Sample the data to reduce training time for a quick test\n",
        "df_sampled = df_raw.sample(frac=0.02, random_state=42)\n",
        "X_train = df_sampled['text'].to_numpy()\n",
        "y_train = df_sampled['label'].to_numpy()\n",
        "\n",
        "print(f\"Sampled {len(X_train)} records for training.\")\n",
        "\n",
        "# COMMAND ----------\n",
        "# Cell 4: Build the Keras Model\n",
        "\n",
        "# Define the text vectorization layer parameters\n",
        "max_features = 10000\n",
        "sequence_length = 250\n",
        "\n",
        "# Initialize and adapt the vectorization layer\n",
        "vectorize_layer = TextVectorization(\n",
        "    max_tokens=max_features,\n",
        "    output_mode='int',\n",
        "    output_sequence_length=sequence_length)\n",
        "vectorize_layer.adapt(X_train)\n",
        "\n",
        "# Build the Sequential model\n",
        "model = Sequential([\n",
        "    vectorize_layer,\n",
        "    Embedding(max_features + 1, 128),\n",
        "    Bidirectional(LSTM(64)),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# COMMAND ----------\n",
        "# Cell 5: Train and Manually Log Metrics to MLflow\n",
        "\n",
        "with mlflow.start_run(run_name=\"local_manual_logging_run\") as run:\n",
        "    print(\"Starting BiLSTM model training with manual logging...\")\n",
        "\n",
        "    # Train the model and get the training history\n",
        "    history = model.fit(X_train, y_train,\n",
        "                        epochs=3,\n",
        "                        validation_split=0.2,\n",
        "                        batch_size=32)\n",
        "\n",
        "    # Manually log metrics from the history object for each epoch\n",
        "    for i in range(len(history.history['accuracy'])):\n",
        "        mlflow.log_metric(\"accuracy\", history.history['accuracy'][i], step=i)\n",
        "        mlflow.log_metric(\"loss\", history.history['loss'][i], step=i)\n",
        "        mlflow.log_metric(\"val_accuracy\", history.history['val_accuracy'][i], step=i)\n",
        "        mlflow.log_metric(\"val_loss\", history.history['val_loss'][i], step=i)\n",
        "\n",
        "    # Log the final trained model\n",
        "    mlflow.keras.log_model(model, \"bilstm-model\")\n",
        "\n",
        "    print(\"\\nTraining complete and manually logged to local MLflow.\")\n",
        "\n",
        "# COMMAND ----------\n",
        "# Cell 6: View the MLflow UI\n",
        "#\n",
        "# After this cell finishes, you need to open a terminal in Colab and run\n",
        "# the following command to start the MLflow UI.\n",
        "#\n",
        "# !mlflow ui --backend-store-uri file:///mlruns\n",
        "\n",
        "print(\"To view the MLflow UI, run `!mlflow ui` in a new Colab cell or terminal.\")\n",
        "print(\"You will need to use a tool like ngrok to expose the web page publicly, if needed.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H0VMYjnplZda"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}